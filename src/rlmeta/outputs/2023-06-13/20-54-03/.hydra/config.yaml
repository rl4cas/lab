env_config:
  length_violation_reward: -2.0
  double_victim_access_reward: -0.01
  victim_access_reward: -0.01
  correct_reward: 1.0
  wrong_reward: -1.0
  step_reward: -0.01
  verbose: 0
  force_victim_hit: false
  flush_inst: false
  allow_victim_multi_access: true
  allow_empty_victim_access: false
  attacker_addr_s: 4
  attacker_addr_e: 7
  victim_addr_s: 0
  victim_addr_e: 3
  reset_limit: 1
  cache_configs:
    architecture:
      word_size: 1
      block_size: 1
      write_back: true
    cache_1:
      blocks: 4
      associativity: 1
      hit_time: 1
      rep_policy: lru
    mem:
      hit_time: 1000
  window_size: 64
model_config:
  type: transformer
  args:
    latency_dim: 3
    victim_acc_dim: 2
    action_dim: 64
    step_dim: 64
    action_embed_dim: 16
    step_embed_dim: 4
    hidden_dim: 128
    output_dim: -1
    num_layers: 1
m_server_name: m_server
m_server_addr: 127.0.0.1:4411
r_server_name: r_server
r_server_addr: 127.0.0.1:4412
c_server_name: c_server
c_server_addr: 127.0.0.1:4413
train_device: cuda:0
infer_device: cuda:0
num_train_rollouts: 4
num_train_workers: 2
num_eval_rollouts: 2
num_eval_workers: 1
replay_buffer_size: 1024
prefetch: 2
batch_size: 512
optimizer:
  name: Adam
  lr: 0.0001
learning_starts: 65536
model_push_period: 10
entropy_coeff: 0.02
num_epochs: 1000
steps_per_epoch: 3000
num_eval_episodes: 100
seed: null
table_view: false
